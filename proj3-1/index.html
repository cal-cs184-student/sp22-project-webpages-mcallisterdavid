<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>184 Project 3-1</title>
	<link rel="stylesheet" href="style.css" />
</head>

<body class="stackedit">
<div class="stackedit__html"><h1 id="cs-184-computer-graphics-and-imaging-spring-2022">Project 3-1: Pathtracter 1</h1>
	<h3 id="project-3-1">CS 184: Computer Graphics and Imaging, Spring 2022</h3>
	<h3 id="david-mcallister-and-james-dai">David McAllister and James Dai</h3>
	<h2 id="task-1">Part 1: Ray Generation and Scene Intersection</h2>
	<p>Ray generation involves generating rays that originate at the sensor and pass through the sensor at various locations within each pixel. These locations are uniformly distributed across each pixel, allowing us to send more samples to better approximate the expected image.</p>
	<p>Ray-triangle intersection has multiple parts. The first is to calculate the time <em>t</em> of intersection. This, however, does not always tell us if it actually intersects the geometry, so we need to calculate the barycentric coordinates of the hit point. If any of these are less than 0, we know there is no intersection. If they are all above zero and sum to one, there is an intersection and we can use these coordinates to interpolate a normal shading.</p>
	<p>Ray-sphere intersection is quite a bit simpler. We get two intersection times, which specify the entry and exit times. If the ray is perfectly tangent to the sphere, we would only get a single intersection time. These values allow us to determine if the intersection was valid.</p>
	<p>Regardless of the intersection type, we only want to intersect the nearest geometry, so we update the ray's <em>max_t</em> and only observe intersections with lower, still positive <em>t</em> values.</p>
	<p><img src="part1/CBspheres.png"><br>
		<em>CBspheres_lambertian</em> with normal shading and no BVH acceleration.</p>
	<p><img src="part1/teapot.png"><br>
		<em>teapot.dae</em> with normal shading and no BVH acceleration.</p>
	<p><img src="part1/cow.png"><br>
		<em>cow.dae</em> with normal shading and no BVH acceleration.</p>
	<h2 id="task-2">Part 2: Bounding Volume Hierarchy</h2>
	<ul>
	</ul>
	<p>For the BVH algorithm, we first found the centroid of the expanded bounding box that covers all the primitives. We then utilized this point to calculate how many primitives would be on the left and right of the x, y, and z axes respectively. This heuristic allowed us to essentially calculate which axis would allow us to get the best split of the primitives by calculating the minimum of the absolute value of the # of primitives on the left of a given axis minus the # of primitives on the right of a given axis. Utilizing this new split, we created two vectors and appended all the primitives that are on the left and right of the split line to their corresponding vectors. Finally, we used these vectors to partition the original primitives vector, shuffling it so the left primitives are on the left side and the right primitives are on the right. We then recurse with the pointers corresponding to the start and end of the left primitives and the pointers corresponding to the start and end of the right primitives. When the size of the split is less than max_leaf_size, we can terminate the recursion.</p>

	<p><b>BVH-accelerated and non-accelerated normal shading render times for various input files:</b></p>

	<table>
	<thead>
	<tr>
	<th>File</th>
	<th>Non-BVH (s)</th>
	<th>BVH (s)</th>
	</tr>
	</thead>
	<tbody>
	<tr>
	<td>sky/CBspheres_lambertian.dae</td>
	<td>0.1426</td>
	<td>0.0995</td>
	</tr>
	<tr>
	<td>meshedit/cow.dae</td>
	<td>42.8786</td>
	<td>0.0778</td>
	</tr>
	<tr>
	<td>meshedit/teapot.dae</td>
	<td>17.7097</td>
	<td>0.1699</td>
	</tr>
	</tbody>
	</table>

	<br />
	<p><b>Normal-shaded renders of complex geometry that would be infeasible without BVH-acceleration:</b></p>

	<p><img src="bvh/bvh_dragon.png"><br>
		<em>dragon.dae</em> rendered with normal shading.</p>
	<p><img src="bvh/bvh_blob.png"><br>
		<em>blob.dae</em> rendered with normal shading.</p>
	<p><img src="bvh/angel_close.png"><br>
		<em>CBlucy.dae</em> rendered with normal shading and adjusted perspective.</p>

	<h2 id="task-3">Part 3: Direct Illumination</h2>
	<p>For the <em>estimate_direct_lighting_hemisphere</em> function, we uniformly sampled a ray from the CosineWeightedHemisphere. Since the hemisphere sampler is in object space, we transform it into world space and use our BVH data structure to see if the world vector intersects with any primitives in <em>log(n)</em> time. If it does, we sample the BSDF function with w_out and our sampled w_in (all from object space) to see how much light is reflected towards w_out (i.e. the camera in the case of a primary ray). We normalize by the PDF of selecting this direction (in this case a factor of pi since it is uniform) and the BSDF function. Finally, we aggregate this into a total light and average it out.</p>

	<p>The <em>estimate_direct_lighting_importance</em> function is fairly similar but now utilizes the locations of the lights more directly. We loop over all the lights in our scene and sample the light radiance that will hit our hit point using the <em>sample_L</em> function. We do this a number of times per light specified by the <em>-l</em> parameter. We then similarly calculate the reflection from the hemisphere function above to aggregate the light. Importance lighting allows us to also sample point lights, which don't work with hemisphere sampling since the likelihood of a random ray intersecting it is zero.</p>
		
	<p><b>Direct illumination hemisphere vs. importance comparison:</b></p>
	<p><img src="direct_vs_hemisphere_s8_l16/bunny_hemi.png"><br>
		<em>bunny.dae</em> rendered with 8 samples per pixel and 16 hemisphere samples per light.</p>
	<p><img src="direct_vs_hemisphere_s8_l16/bunny_direct.png"><br>
		<em>bunny.dae</em> rendered with 8 samples per pixel and 16 importance samples per light.</p>

	<p><b>Soft shadow comparison with more importance samples per light.</b></p>
	<p><img src="direct_illum_l_levels/bunny_l1.png"><br>
		<em>bunny.dae</em> rendered with 1 sample per pixel and 1 sample per light.</p>
	<p><img src="direct_illum_l_levels/bunny_l4.png"><br>
		<em>bunny.dae</em> rendered with 1 sample per pixel and 4 sample per light.</p>
	<p><img src="direct_illum_l_levels/bunny_l16.png"><br>
		<em>bunny.dae</em> rendered with 1 sample per pixel and 16 sample per light.</p>
	<p><img src="direct_illum_l_levels/bunny_l64.png"><br>
		<em>bunny.dae</em> rendered with 1 sample per pixel and 64 sample per light.</p>
	<p>As shown in the images, increasing the samples per light dramatically reduces noise in the soft shadows under the bunny. It also lower noise in non-shadow areas, but less noticeably.</p>

	<p>Importance sampling way outperforms hemisphere sampling. While hemisphere sampling hopes to intersect all light sources enough times to render acceptable shadows, importance sampling only intersects light sources and geometry directly blocking light sources. This greatly reduces noise in shadows as well as directly illuminated sections of the image. While both will converge to the expected image with enough samples, importance sampling will get there much more quickly. In this way, it makes low-noise pathtracing computationally tractable.</p>

	<h2 id="task-4">Part 4: Global Illumination</h2>
	<p>Our implementation of the indirect lighting function wraps around our implementation of the direct lighting function. It casts rays which intersect geometry and compute the "one-bounce" or direct lighting at that intersection. Instead of stopping there, we uniformly sample a unit hemisphere and send out another ray to do the same. This ray's returned illumination is then normalized by its angle of incidence, probability of being cast in that direction, and intersection's reflective properties. This recursive ray generation can be terminated in two ways. First, we specify a <em>max ray depth</em> at which rays stop being generated. Second, we introduce a termination probability of 0.3 at each bounce, implementing russian roulette. This introduces an additional normalization factor and returns our estimation to being unbiased.</p>

	<p><b>Direct vs. global illumination:</b></p>
	<p><img src="part4-1/direct_1024.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 1024 samples per pixel and only direct illumination.</p>
	<p><img src="part4-1/global_1024.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 1024 samples per pixel and both direct and indirect illumination.</p>

	<br />
	<p><b>Only direct vs. only indirect illumination:</b></p>
	<p><img src="part4-only_direct_and_only_indirect/only_direct.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 1024 samples per pixel and only direct illumination.</p>
	<p><img src="part4-only_direct_and_only_indirect/only_indirect.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 1024 samples per pixel and only indirect illumination.</p>
	<br />
	<p><b>Max ray depth comparison (-m):</b></p>

	<p><img src="mcomparison/bunny0.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel and a max ray depth of 0 (zero-bounce).</p>
	<p><img src="mcomparison/bunny1.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel and a max ray depth of 1 (direct lighting).</p>
	<p><img src="mcomparison/bunny2.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel and a max ray depth of 2.</p>
	<p><img src="mcomparison/bunny3.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel and a max ray depth of 3.</p>
	<p><img src="mcomparison/bunny100.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel and a max ray depth of 100.</p>

	<br />
	<p><b>Pixel sample rate comparison (-s):</b></p>
	<p><img src="part4-s_comparison/s1.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 1 sample per pixel.</p>
	<p><img src="part4-s_comparison/s2.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 2 samples per pixel.</p>
	<p><img src="part4-s_comparison/s4.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 4 samples per pixel.</p>
	<p><img src="part4-s_comparison/s8.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 8 samples per pixel.</p>
	<p><img src="part4-s_comparison/s16.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 16 samples per pixel.</p>
	<p><img src="part4-s_comparison/s64.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with 64 samples per pixel.</p>
	<p><img src="part4-s_comparison/s1024.png"><br>
		<em>bunny.dae</em> rendered with 1024 samples per pixel.</p>


	<h2 id="task-5">Part 5: Adaptive Sampling</h2>
	<p>We created two variables s1 and s2, which essentially aggregates the illuminance and illuminance squared respectively. We then calculate the mean and variance by simply following the formulas from the spec. We then calculate the interval and ensure it is less than our max_tolerance * mean. If it is, then we are 95% confident that the pixel lighting has converged. Therefore, we can break out of our loop early. This allows more computational resources to be applied to parts of the image that are the most difficult to compute.</p>
	<p><b>Adaptive sampling rate over image:</b></p>
	<p><img src="part5/part5-try3.png"><br>
		<em>CBspheres_lambertian.dae</em> rendered with a maximum of 2048 samples per pixel, 1 sample per light, and 5 samples per light.</p>
	<p><img src="part5/part5-try3_rate.png"><br>
		Visualization of adaptive sample rate across the image, with red representing higher rates and blue representing lower rates.</p>

	<h2 id="task-6">Extra: WALL-E</h2>
	<p>We rendered Wall-E in 1080p.</p>
	<p><img src="walle/wall-e_hd.png"></p>
	<p><em>CSS styling provided by StackEdit Markdown to HTML.</em> <a href="https://stackedit.io"> stackedit.io</p>
</div>
</body>

</html>
